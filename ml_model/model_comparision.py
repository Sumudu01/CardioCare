# -*- coding: utf-8 -*-
"""model_comparision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19sasldJlgHz_bqLwRZF6-w1rliOO_6p_
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# --- 1. Load and Preprocess Data ---

# Load the original dataset
df = pd.read_csv("/content/drive/MyDrive/heart_attack_train_processed.csv")

# Separate features (X) and target (y)
X = df.drop('heart_attack', axis=1)
y = df['heart_attack']

# Identify categorical features and apply One-Hot Encoding
categorical_cols = X.select_dtypes(include=['object']).columns
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Define target names for output reports
target_names = ['No Attack', 'Heart Attack']

# Split data into training and testing sets (70% train, 30% test)
# stratify=y ensures equal class distribution of the binary target
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)

# Standardize features (Scaling is vital for SVM and Neural Networks)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# For Binary Classification, y is not one-hot encoded for NN
# The final number of input features is X_train_scaled.shape[1] (which is 52)

print(f"  Data loaded and preprocessed. Total features: {X_train_scaled.shape[1]}")
print("---------------------------------------")

# --- 2. Train and Evaluate Support Vector Machine (SVM) ---

print(" Training Support Vector Machine (SVM)...")

# Create and train the SVM model (using Radial Basis Function kernel)
svm_model = SVC(kernel='rbf', random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
svm_y_pred = svm_model.predict(X_test_scaled)

# Calculate metrics for SVM (using 'weighted' average for robust summary)
svm_accuracy = accuracy_score(y_test, svm_y_pred)
svm_precision_weighted = precision_score(y_test, svm_y_pred, average='weighted')
svm_recall_weighted = recall_score(y_test, svm_y_pred, average='weighted')
svm_f1_weighted = f1_score(y_test, svm_y_pred, average='weighted')

print("\n--- SVM Model Evaluation ---")
print(f"Accuracy: {svm_accuracy:.4f}")
print(f"Precision (weighted): {svm_precision_weighted:.4f}")
print(f"Recall (weighted): {svm_recall_weighted:.4f}")
print(f"F1-Score (weighted): {svm_f1_weighted:.4f}")
print("\nClassification Report:")
# Using zero_division=0 to prevent warnings/errors if a class is completely missed
print(classification_report(y_test, svm_y_pred, target_names=target_names, zero_division=0))
print("---------------------------------------")

# --- 3. Train and Evaluate Neural Network (NN) ---

print(" Training Neural Network (NN) - Simple Multi-Layer Perceptron (Binary)...")

# Define the Neural Network model for binary classification
nn_model = Sequential([
    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)), # 1st hidden layer
    Dense(16, activation='relu'),                                         # 2nd hidden layer
    # Output layer: 1 neuron with sigmoid for binary output (0 or 1)
    Dense(1, activation='sigmoid')
])

# Compile the model
nn_model.compile(optimizer='adam',
                 loss='binary_crossentropy', # Appropriate loss for binary classification
                 metrics=['accuracy'])

# Train the model (50 epochs)
history = nn_model.fit(X_train_scaled, y_train, # Use non-one-hot encoded y
                       epochs=50,
                       batch_size=32,
                       validation_split=0.1,
                       verbose=0) # Suppress training log output for cleaner notebook

# Make predictions on the test set (outputs probabilities)
nn_y_prob = nn_model.predict(X_test_scaled, verbose=0)
# Convert probabilities to class labels (0 or 1) using a 0.5 threshold
nn_y_pred = (nn_y_prob > 0.5).astype(int)

# Calculate metrics for NN
nn_accuracy = accuracy_score(y_test, nn_y_pred)
nn_precision_weighted = precision_score(y_test, nn_y_pred, average='weighted', zero_division=0)
nn_recall_weighted = recall_score(y_test, nn_y_pred, average='weighted', zero_division=0)
nn_f1_weighted = f1_score(y_test, nn_y_pred, average='weighted', zero_division=0)

print("\n--- Neural Network Model Evaluation ---")
print(f"Accuracy: {nn_accuracy:.4f}")
print(f"Precision (weighted): {nn_precision_weighted:.4f}")
print(f"Recall (weighted): {nn_recall_weighted:.4f}")
print(f"F1-Score (weighted): {nn_f1_weighted:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, nn_y_pred, target_names=target_names, zero_division=0))
print("---------------------------------------")

# --- 4. Comparison Summary and Visualization ---

# Create a DataFrame for easy comparison
data = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],
    'SVM': [svm_accuracy, svm_precision_weighted, svm_recall_weighted, svm_f1_weighted],
    'Neural Network': [nn_accuracy, nn_precision_weighted, nn_recall_weighted, nn_f1_weighted]
}
df_metrics = pd.DataFrame(data)

print("\n Comparison Summary Table:")
print(df_metrics.set_index('Metric').round(4))
print("---------------------------------------")

# Create a Grouped Bar Chart for Visualization
metrics = df_metrics['Metric']
svm_scores = df_metrics['SVM']
nn_scores = df_metrics['Neural Network']

x = np.arange(len(metrics)) # the label locations
width = 0.35 # the width of the bars

fig, ax = plt.subplots(figsize=(10, 6))
rects1 = ax.bar(x - width/2, svm_scores, width, label='SVM (RBF)', color='#1f77b4')
rects2 = ax.bar(x + width/2, nn_scores, width, label='Neural Network', color='#ff7f0e')

# Customizing the plot
ax.set_ylabel('Weighted Score')
ax.set_title('Model Performance Comparison (SVM vs. Neural Network)')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()
# Adjust Y-axis dynamically for better visualization
ax.set_ylim(min(svm_scores.min(), nn_scores.min()) * 0.9, 1.05)

# Function to attach a label on each bar
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom',
                    fontsize=8)

autolabel(rects1)
autolabel(rects2)

plt.tight_layout()
plt.show() # Display the plot

#Logistic Regression vs Baseline (Colab single cell) ===
# Upload CSV -> preprocess (numeric + categorical) -> train/test split
# -> train Logistic Regression -> train Majority Baseline -> compare metrics

import io, re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_auc_score, roc_curve, ConfusionMatrixDisplay
)

# ---------- 1) Upload CSV ----------
try:
    from google.colab import files  # works in Colab
    print("ðŸ“¤ Please upload your CSV (e.g., heart_attack_train_processed.csv)")
    uploaded = files.upload()
    csv_name = next(k for k in uploaded.keys() if k.lower().endswith(".csv"))
    df = pd.read_csv(io.BytesIO(uploaded[csv_name]))
    print(f"âœ… Loaded {csv_name}  shape={df.shape}")
except Exception as e:
    raise RuntimeError(f"CSV upload/read failed: {e}")

# Drop accidental index columns like 'Unnamed: 0'
for c in list(df.columns):
    if re.fullmatch(r"unnamed:\s*0", str(c).strip(), flags=re.I):
        df = df.drop(columns=[c])

# ---------- 2) Target detection & cleaning ----------
def find_col(dataframe, candidates):
    cols_norm = {c: re.sub(r"\s+", "_", str(c)).strip().lower() for c in dataframe.columns}
    # exact first
    for orig, norm in cols_norm.items():
        if norm in candidates: return orig
    # substring next
    for orig, norm in cols_norm.items():
        if any(cand in norm for cand in candidates): return orig
    return None

target_candidates = {
    "heart_attack","heartattack","has_heart_attack","heart_disease",
    "target","label","outcome","disease","class"
}
target_col = find_col(df, target_candidates)
assert target_col is not None, (
    f"Couldn't find target column. Please rename your label to one of: {sorted(target_candidates)}"
)
y_raw = df[target_col]

# Map common yes/no style labels to 0/1 if needed
if not np.issubdtype(y_raw.dtype, np.number):
    mapping = {
        "yes":1,"y":1,"true":1,"present":1,"positive":1,"pos":1,"1":1,
        "no":0,"n":0,"false":0,"absent":0,"negative":0,"neg":0,"0":0
    }
    y = y_raw.astype(str).str.strip().str.lower().map(mapping)
else:
    y = y_raw.copy()

# Final cast & sanity check
try:
    y = y.astype("Int64")
except Exception:
    pass
uniq = pd.Series(y.dropna().unique())
assert uniq.isin([0,1]).all(), f"Target looks non-binary: {uniq.tolist()}"

# Features
X = df.drop(columns=[target_col])

# ---------- 3) Train/test split ----------
Xtr, Xte, ytr, yte = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("\nðŸ”¢ Class balance (overall):")
print(y.value_counts(normalize=True).rename("proportion").round(3))

# ---------- 4) Preprocessing (numeric + categorical) ----------
num_cols = Xtr.select_dtypes(include=np.number).columns.tolist()
cat_cols = [c for c in Xtr.columns if c not in num_cols]

# OneHotEncoder compat across sklearn versions
try:
    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
except TypeError:
    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)

numeric_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])
categorical_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("ohe", ohe)
])

preprocessor = ColumnTransformer([
    ("num", numeric_tf, num_cols),
    ("cat", categorical_tf, cat_cols)
])

# ---------- 5) Models ----------
logreg = Pipeline([
    ("prep", preprocessor),
    ("clf", LogisticRegression(max_iter=1000))
])

baseline = Pipeline([
    ("prep", preprocessor),  # not required for Dummy, but keeps interface consistent
    ("clf", DummyClassifier(strategy="most_frequent"))
])

# ---------- 6) Train ----------
logreg.fit(Xtr, ytr)
baseline.fit(Xtr, ytr)

# ---------- 7) Evaluate ----------
def evaluate(model, name, Xte, yte, proba_ok=True):
    yhat = model.predict(Xte)
    acc = accuracy_score(yte, yhat)
    report = classification_report(yte, yhat, digits=3)
    print(f"\n=== {name} ===")
    print(f"Accuracy: {acc:.3f}")
    print(report)
    # Confusion matrix
    ConfusionMatrixDisplay.from_predictions(yte, yhat)
    plt.title(f"{name} â€“ Confusion Matrix")
    plt.show()
    # ROC-AUC (binary) for models with probabilities
    if proba_ok:
        try:
            yproba = model.predict_proba(Xte)[:, 1]
            auc = roc_auc_score(yte, yproba)
            fpr, tpr, _ = roc_curve(yte, yproba)
            plt.plot(fpr, tpr, label=f"{name} (AUC={auc:.3f})")
            plt.plot([0,1],[0,1], linestyle="--")
            plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
            plt.title(f"{name} â€“ ROC Curve"); plt.legend(); plt.grid(True); plt.show()
        except Exception as e:
            print(f"(No ROC-AUC shown: {e})")

evaluate(baseline, "Baseline (Majority Class)", Xte, yte, proba_ok=False)
evaluate(logreg,   "Logistic Regression",      Xte, yte, proba_ok=True)

print("\nâœ… Done. If performance seems low, consider: feature engineering, class_weight='balanced', "
      "or threshold tuning using predicted probabilities.")

# === Decision Tree & Random Forest ===
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load cleaned dataset (upload once via left panel or files.upload())
RAW = "/content/drive/MyDrive/heart_attack_train_processed.csv"
df = pd.read_csv(RAW)

print("Shape:", df.shape)
df.head()

#Split Features & Target

# Identify target column
target = "heart_attack"

X = df.drop(columns=[target])
y = df[target]

# Encode categorical variables
X_encoded = X.copy()
for col in X_encoded.select_dtypes(include=["object", "string"]).columns:
    le = LabelEncoder()
    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))

# Scale numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print("Train size:", X_train.shape, "Test size:", X_test.shape)

#Train Decision Tree

# Decision Tree
dt = DecisionTreeClassifier(random_state=42, max_depth=5)  # limit depth for interpretability
dt.fit(X_train, y_train)

y_pred_dt = dt.predict(X_test)

print("=== Decision Tree Report ===")
print(classification_report(y_test, y_pred_dt))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_dt)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Decision Tree Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Plot tree (simplified)
plt.figure(figsize=(15,8))
plot_tree(dt, filled=True, feature_names=X_encoded.columns, class_names=["No","Yes"], max_depth=3)
plt.show()

#Train Random Forest

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)

print("=== Random Forest Report ===")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens")
plt.title("Random Forest Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Feature importance
importances = pd.Series(rf.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)
plt.figure(figsize=(10,6))
sns.barplot(x=importances[:15], y=importances.index[:15])
plt.title("Top 15 Feature Importances (Random Forest)")
plt.show()

#Compare Performance

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))